{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ebab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea642315",
   "metadata": {},
   "source": [
    "PREPROCESSING/EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "data = pd.read_csv('/tmp/insurance.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding categorical vs numerical (can also use df.dtypes)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicated values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = data[data.duplicated()]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling duplicates\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fcf564",
   "metadata": {},
   "source": [
    "don't do label encoding which is assigning a number to each feature; it messes up a neural network; changing to one hot encoding which makes dummy variables for each feature (sql distinct comes in clutch here) and then uses binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_region = pd.get_dummies(data.region)\n",
    "ohe_sex = pd.get_dummies(data.sex)\n",
    "ohe_smoker = pd.get_dummies(data.smoker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f66ac1",
   "metadata": {},
   "source": [
    "ADVANCED FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical => categorical for \"imbalance\"/skew purposes; THIS IS CALLED BINNING\n",
    "data['age_category'] = pd.cut(data['age'], bins=[0, 25, 35, 45, 55, 65], labels=['18-25', '26-35', '36-45', '46-55', '56-65'])\n",
    "data['bmi_category'] = pd.cut(data['bmi'], bins=[0, 18.5, 25, 30, np.inf], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "data['charges_category'] = pd.cut(data['charges'], bins=[0, 10000, 20000, 30000, 40000, 50000, 60000, np.inf], labels=['$0-10,000', '$10,001-20,000', '$20,001-30,000', '$30,001-40,000', '$40,001-50,000', '$50,001-60,000', '$60,000+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focusing on smokers as correlation matrix indicated strong relationship\n",
    "data['elderly_smoker'] = ((data['smoker'] == 'yes') & (data['age'] > 39)).astype(int)\n",
    "data['obese_smoker'] = ((data['smoker'] == 'yes') & (data['bmi_category'] == 'Obese')).astype(int)\n",
    "\n",
    "# family size impact (testing)\n",
    "data['has_children'] = ((data['children'] > 0)).astype(int)\n",
    "data['largeFamily'] = ((data['children'] >= 3)).astype(int)\n",
    "\n",
    "# MORE\n",
    "data['age_squared'] = data['age'] ** 2\n",
    "data['log_bmi'] = np.log1p(data['bmi'])\n",
    "data['log_charges'] = np.log1p(data['charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a52fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, ohe_region, ohe_sex, ohe_smoker], axis='columns')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce91fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cc813",
   "metadata": {},
   "source": [
    "BEGINNING THE VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d112d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all my plots\n",
    "palette = [ '#C66F80', '#F4C7D0','#FCEBF1', '#4A6644', '#9FAA74', '#D7DAB3', '#ECE3D2']\n",
    "customCmap = ListedColormap(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "correlation_matrix = data.select_dtypes(include=[\"number\", \"bool\"]).corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=customCmap, \n",
    "            fmt='.3f', square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix - All Numeric & Boolean Features',)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations with Charges (target):\")\n",
    "print(correlation_matrix['charges'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c2e2f",
   "metadata": {},
   "source": [
    "strong correlation between smoking & insurance charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "histogram = data.select_dtypes(include=[\"number\", \"bool\"]).hist(bins=15, figsize=[15,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d696f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['yes'] == True]['charges'].hist(bins=15, figsize=[8,6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922c438",
   "metadata": {},
   "source": [
    "age, children, and charges are skewed to the right. bmi is reminscent of a normal curve. There is evidence of imbalance in age, children, and smoker features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eeac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to charge feature skew, most outliers are on the higher end. \n",
    "sns.boxplot(y='charges', x='region', hue='smoker', data=data, palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a59abd",
   "metadata": {},
   "source": [
    "multiple experiments with boxplots; can make numerous conclusions about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db434a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot video experimentation; this is so large bc of the boolean & numerical features; not super important in the grand scheme of things\n",
    "sns.pairplot(data, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588db37b",
   "metadata": {},
   "source": [
    "- detecting numerical features\n",
    "- histograms on the diagonals; scatter plots everywhere else; can be changed\n",
    "- use hue for categorical features\n",
    "- boolean types are treated as numeric\n",
    "- can specify which exact variables you want to see & which axis they're on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='smoker', palette=palette, vars=['charges', 'bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74573059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand a misconception\n",
    "sns.kdeplot(data, x='age', hue='smoker', palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a03de",
   "metadata": {},
   "source": [
    "learned quite a lot from that; despite the normal-looking curve for bmi, the results showed heavy skew for overweight & obese individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70364e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='charges', x='sex', hue='bmi_category', data=data, palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39852fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a contingency table to see sex & bmi broken down\n",
    "crosstab01 = pd.crosstab(data['sex'], data['bmi_category'])\n",
    "crosstab01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of above\n",
    "crosstab01.plot(kind='bar', stacked = True, colormap=customCmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking feature importance (i think??); seeing if i can see any distinct noticeable patterns\n",
    "columns = ['age_category', 'sex', 'bmi_category', 'smoker', 'children', 'region']\n",
    "\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=col, hue='charges_category', data=data, palette=palette)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Insurance Charges', loc='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f15d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting but knew from sql queries as well as really only helpful for visualizations & classification problems\n",
    "data['charges_category'].value_counts().plot(kind='bar', colormap=customCmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO SKEWED\n",
    "data['charges_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding average features for each charge category\n",
    "numeric_columns = ['age', 'children', 'bmi']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    mean_values = data.groupby('charges_category')[col].mean()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mean_values.plot(kind='bar', color=customCmap.colors)\n",
    "    plt.title(f'Average {col.capitalize()} per Insurance Charge Category')\n",
    "    plt.xlabel('Charge Category')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel(col.capitalize())\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9ea20",
   "metadata": {},
   "source": [
    "MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data = data.drop(['sex', 'smoker', 'region', 'charges_category', 'age_category', 'bmi_category', 'children', 'age', 'bmi', 'charges'], axis=1)\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65dfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature & target split\n",
    "x = modeling_data.drop('log_charges', axis=1)\n",
    "y = modeling_data['log_charges']\n",
    "\n",
    "# test train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "y_train = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test = scaler.transform(y_test.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7e8a8",
   "metadata": {},
   "source": [
    "going to compare random forest (regressor) & regression neural network & decision tree (regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with decision tree\n",
    "dt_model = DecisionTreeRegressor(max_depth=5, min_samples_split=2, min_samples_leaf=10, criterion='absolute_error', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d95f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt_model.predict(x_test)\n",
    "y_pred_dt = scaler.inverse_transform(y_pred_dt.reshape(-1, 1))\n",
    "y_pred_dt = np.expm1(y_pred_dt)\n",
    "\n",
    "y_true_dt = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_true_dt = np.expm1(y_true_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02212438",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dt = mean_squared_error(y_true_dt, y_pred_dt)\n",
    "mae_dt = mean_absolute_error(y_true_dt, y_pred_dt)\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "r2_dt = r2_score(y_true_dt, y_pred_dt)\n",
    "\n",
    "print(f\"Decision Tree - Mean Squared Error: {mse_dt:.4f}\")\n",
    "print(f\"Decision Tree - Mean Absolute Error: {mae_dt:.4f}\")\n",
    "print(f\"Decision Tree - Root Mean Squared Error: {rmse_dt:.4f}\")\n",
    "print(f\"Decision Tree - R² Score: {r2_score(y_true_dt, y_pred_dt):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_charges = 13279\n",
    "mae_pct_dt = mae_dt / mean_charges * 100\n",
    "rmse_pct_dt = rmse_dt / mean_charges * 100\n",
    "print(f\"MAE is about {mae_pct_dt:.1f}% of the average cost\")\n",
    "print(f\"RMSE is about {rmse_pct_dt:.1f}% of the average cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4283092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance for decision forests\n",
    "importance = dt_model.feature_importances_\n",
    "x_names = ['elderly_smoker', 'obese_smoker', 'has_children', 'largeFamily',\t'age_squared',\t'log_bmi',\t'northeast',\t'northwest',\t'southeast',\t'southwest',\t'female',\t'male',\t'no',\t'yes']\n",
    "feature_importances = list(zip(x_names, importance))\n",
    "feature_importances_sorted = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "sorted_names, sorted_importance = zip(*feature_importances_sorted)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_importance)), sorted_importance, align='center', color=palette)\n",
    "plt.yticks(np.arange(len(sorted_importance)), sorted_names)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance of Decision Tree Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding residual plot\n",
    "y_pred_dt = np.ravel(y_pred_dt)\n",
    "y_true_dt = np.ravel(y_true_dt)\n",
    "residuals = y_true_dt - y_pred_dt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_pred_dt, y=residuals)\n",
    "plt.axhline(0, color='pink', linestyle='--')\n",
    "plt.xlabel('Predicted Charges')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot of Decision Tree Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7be8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding actual vs predicted plot\n",
    "plt.scatter(y_true_dt, y_pred_dt, alpha = 0.4)\n",
    "plt.plot([y_true_dt.min(), y_true_dt.max()], [y_true_dt.min(), y_true_dt.max()], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning of error analysis\n",
    "errors = y_true_dt.flatten() - y_pred_dt.flatten()\n",
    "abs_errors = np.abs(errors)\n",
    "pct_errors = (abs_errors / y_true_dt.flatten()) * 100\n",
    "\n",
    "# create dataframe\n",
    "error_data_dt = pd.DataFrame({\n",
    "    'true_value': y_true_dt.flatten(),\n",
    "    'predicted_value': y_pred_dt.flatten(),\n",
    "    'error': errors,\n",
    "    'abs_error': abs_errors,\n",
    "    'pct_error': pct_errors\n",
    "})\n",
    "\n",
    "x_test_original = pd.DataFrame(\n",
    "    scaler.inverse_transform(x_test),\n",
    "    columns = x_names\n",
    ")\n",
    "\n",
    "error_data_dt = pd.concat([error_data_dt, x_test_original.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data_dt.to_csv('errors_dt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic error statistics\n",
    "error_data_dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing error statistics\n",
    "print(f\"median error: ${np.median(errors):,.2f}\")\n",
    "print(f\"median absolute error: ${np.median(abs_errors):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error percentiles\n",
    "print('=== error percentiles ===')\n",
    "for percentile in [50, 75, 90, 95, 99]:\n",
    "    value = np.percentile(abs_errors, percentile)\n",
    "    print(f'{percentile}th percentile: ${value:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b664534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for bias (systematic open/under prediction)\n",
    "t_stat, p_value = stats.ttest_1samp(errors, 0)\n",
    "\n",
    "print(f\"mean error: ${errors.mean():.2f}\")\n",
    "print(f'as % of average cost: {(errors.mean() / y_true_dt.mean()) * 100:.2f}')\n",
    "\n",
    "if p_value < 0.05 and abs(errors.mean()) > 100:\n",
    "    if errors.mean() > 0:\n",
    "        print(f\"\\n⚠️ model systematically underpredicts (p={p_value:.4f})\")\n",
    "    else: \n",
    "        print(f\"\\n⚠️ model systematically overpredicts (p={p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n✅ No significant systematic bias detected (p={p_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are high-cost cases driving the bias?\n",
    "print(\"\\n=== Errors by Cost Range ===\")\n",
    "error_data_dt['cost_bin'] = pd.cut(error_data_dt['true_value'], \n",
    "                               bins=[0, 5000, 10000, 20000, 100000],\n",
    "                               labels=['<$5k', '$5-10k', '$10-20k', '>$20k'])\n",
    "print(error_data_dt.groupby('cost_bin')['error'].agg(['mean', 'count']))\n",
    "\n",
    "# My guess: You're underpredicting HIGH-cost cases significantly\n",
    "# Which pulls up the overall mean error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true_dt, errors, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('True Value ($)')\n",
    "plt.ylabel('Error (True - Predicted) ($)')\n",
    "plt.title('Error Pattern: Underprediction Increases with Cost')\n",
    "plt.show()\n",
    "\n",
    "# I bet you'll see errors trending upward as true value increases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at this more carefully\n",
    "print(\"=== Detailed Error Analysis by Cost Range ===\\n\")\n",
    "\n",
    "for range_name in ['<$5k', '$5-10k', '$10-20k', '>$20k']:\n",
    "    range_data = error_data_dt[error_data_dt['cost_bin'] == range_name]\n",
    "    \n",
    "    print(f\"\\n{range_name}:\")\n",
    "    print(f\"  Count: {len(range_data)}\")\n",
    "    print(f\"  Mean error: ${range_data['error'].mean():,.2f}\")\n",
    "    print(f\"  Median error: ${range_data['error'].median():,.2f}\")  # KEY METRIC!\n",
    "    print(f\"  Std dev: ${range_data['error'].std():,.2f}\")\n",
    "    print(f\"  Max error: ${range_data['error'].max():,.2f}\")\n",
    "    print(f\"  Min error: ${range_data['error'].min():,.2f}\")\n",
    "    \n",
    "    # How many are actually way off?\n",
    "    big_errors = range_data[np.abs(range_data['error']) > 5000]\n",
    "    print(f\"  # with error >$5k: {len(big_errors)} ({len(big_errors)/len(range_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5976a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this\n",
    "print(\"\\n=== Outlier Analysis ===\")\n",
    "threshold = 5000\n",
    "\n",
    "outliers = error_data_dt[np.abs(error_data_dt['error']) > threshold]\n",
    "print(f\"Predictions with error >${threshold}: {len(outliers)} ({len(outliers)/len(error_data_dt)*100:.1f}%)\")\n",
    "print(f\"These {len(outliers)} outliers account for ${outliers['abs_error'].sum():,.0f} of total error\")\n",
    "print(f\"That's {outliers['abs_error'].sum() / error_data_dt['abs_error'].sum() * 100:.1f}% of total error\")\n",
    "\n",
    "print(\"\\nWhat do outliers look like?\")\n",
    "print(outliers[['true_value', 'predicted_value', 'error']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe777a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== WHO ARE THE OUTLIERS? ===\\n\")\n",
    "\n",
    "# Get the 24 outliers\n",
    "outliers = error_data_dt[np.abs(error_data_dt['error']) > 5000]\n",
    "\n",
    "print(\"Outlier Characteristics:\")\n",
    "if 'smoker_yes' in outliers.columns:\n",
    "    print(f\"\\nSmoker percentage:\")\n",
    "    print(f\"  Outliers: {outliers['smoker_yes'].mean()*100:.1f}%\")\n",
    "    print(f\"  Overall: {error_df['smoker_yes'].mean()*100:.1f}%\")\n",
    "\n",
    "if 'bmi' in outliers.columns:\n",
    "    print(f\"\\nAverage BMI:\")\n",
    "    print(f\"  Outliers: {outliers['bmi'].mean():.1f}\")\n",
    "    print(f\"  Overall: {error_df['bmi'].mean():.1f}\")\n",
    "\n",
    "if 'age' in outliers.columns:\n",
    "    print(f\"\\nAverage Age:\")\n",
    "    print(f\"  Outliers: {outliers['age'].mean():.1f}\")\n",
    "    print(f\"  Overall: {error_df['age'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n=== HYPOTHESIS CHECK ===\")\n",
    "# My guess: outliers are mostly smokers with high BMI\n",
    "if 'smoker_yes' in outliers.columns and 'bmi' in outliers.columns:\n",
    "    high_risk = outliers[(outliers['smoker_yes'] == 1) & (outliers['bmi'] > 30)]\n",
    "    print(f\"Outliers that are obese smokers: {len(high_risk)} / {len(outliers)} ({len(high_risk)/len(outliers)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c83c38",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189768e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding hyperparam tuning to random forest \n",
    "n_estimators = [50, 100, 200, 300] # num of trees in random forest\n",
    "max_features = ['auto', 'sqrt', 'log2'] # num of features to consider @ every split\n",
    "max_depth = [4, 8, 12, None] # max num of levels in tree\n",
    "min_samples_split = [2, 5, 10] # min num of samples to split a node\n",
    "min_samples_leaf = [1, 2, 4] # min num of samples to split each leaf node\n",
    "bootstrap = [True] # method of selecting samples for tree training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': n_estimators,\n",
    "                            'max_features': max_features,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf,\n",
    "                            'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd18636",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state = 42)\n",
    "rf_random = RandomizedSearchCV(estimator = rf_model, param_distributions = param_grid, cv = 3, verbose = 2, n_jobs = 1, n_iter = 50, scoring = 'neg_mean_absolute_error', random_state = 42)\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a31abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = best_rf_model.predict(x_test)\n",
    "y_pred_rf = scaler.inverse_transform(y_pred_rf.reshape(-1, 1))\n",
    "y_pred_rf = np.expm1(y_pred_rf)\n",
    "\n",
    "y_true_rf = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_true_rf = np.expm1(y_true_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf = mean_squared_error(y_true_rf, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_true_rf, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_true_rf, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_rf:.4f}\")\n",
    "print(f\"Random Forest - Mean Absolute Error: {mae_rf:.4f}\")\n",
    "print(f\"Random Forest - Root Mean Squared Error: {rmse_rf:.4f}\")\n",
    "print(f\"Random Forest - R² Score: {r2_score(y_true_rf, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ef0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_pct_rf = mae_rf / mean_charges * 100\n",
    "rmse_pct_rf = rmse_rf / mean_charges * 100\n",
    "print(f\"MAE is about {mae_pct_rf:.1f}% of the average cost\")\n",
    "print(f\"RMSE is about {rmse_pct_rf:.1f}% of the average cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance for random forests\n",
    "importance = best_rf_model.feature_importances_\n",
    "x_names = ['elderly_smoker', 'obese_smoker', 'has_children', 'largeFamily',\t'age_squared',\t'log_bmi',\t'northeast',\t'northwest',\t'southeast',\t'southwest',\t'female',\t'male',\t'no',\t'yes']\n",
    "feature_importances = list(zip(x_names, importance))\n",
    "feature_importances_sorted = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "sorted_names, sorted_importance = zip(*feature_importances_sorted)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_importance)), sorted_importance, align='center', color=palette)\n",
    "plt.yticks(np.arange(len(sorted_importance)), sorted_names)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance of Random Forest Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding residual plots\n",
    "y_pred_rf = np.ravel(y_pred_rf)\n",
    "y_true_rf = np.ravel(y_true_rf)\n",
    "residuals = y_true_rf - y_pred_rf\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_pred_rf, y=residuals)\n",
    "plt.axhline(0, color='pink', linestyle='--') \n",
    "plt.xlabel('Predicted Charges')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot of Random Forest Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding actual vs predicted plot\n",
    "plt.scatter(y_true_rf, y_pred_rf, alpha = 0.4)\n",
    "plt.plot([y_true_rf.min(), y_true_rf.max()], [y_true_rf.min(), y_true_rf.max()], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c313f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning of error analysis\n",
    "errors = y_true_rf.flatten() - y_pred_rf.flatten()\n",
    "abs_errors = np.abs(errors)\n",
    "pct_errors = (abs_errors / y_true_rf.flatten()) * 100\n",
    "\n",
    "# create dataframe\n",
    "error_data_rf = pd.DataFrame({\n",
    "    'true_value': y_true_rf.flatten(),\n",
    "    'predicted_value': y_pred_rf.flatten(),\n",
    "    'error': errors,\n",
    "    'abs_error': abs_errors,\n",
    "    'pct_error': pct_errors\n",
    "})\n",
    "\n",
    "x_test_original = pd.DataFrame(\n",
    "    scaler.inverse_transform(x_test),\n",
    "    columns = x_names\n",
    ")\n",
    "\n",
    "error_data_rf = pd.concat([error_data_rf, x_test_original.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data_rf.to_csv('errors_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c79b0",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c496006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple nn arch\n",
    "def build_nn(n_features):\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=(n_features,)),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu', ),\n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63242f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = KerasRegressor(model=build_nn, epochs=20, batch_size=16, verbose=0, model__n_features = x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = nn_model.predict(x_test)\n",
    "y_pred_nn = scaler.inverse_transform(y_pred_nn.reshape(-1, 1))\n",
    "y_pred_nn = np.expm1(y_pred_nn)\n",
    "\n",
    "y_true_nn = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_true_nn = np.expm1(y_true_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68609aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_nn = mean_squared_error(y_true_nn, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_true_nn, y_pred_nn)\n",
    "rmse_nn = np.sqrt(mse_nn)\n",
    "r2_nn = r2_score(y_true_nn, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network - Mean Squared Error: {mse_nn:.4f}\")\n",
    "print(f\"Neural Network - Mean Absolute Error: {mae_nn:.4f}\")\n",
    "print(f\"Neural Network - Root Mean Squared Error: {rmse_nn:.4f}\")\n",
    "print(f\"Neural Network - R² Score: {r2_score(y_true_nn, y_pred_nn):.4f}\") # for PFI, this is baseline score; where no score has been permuted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a588fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_pct_nn = mae_nn / mean_charges * 100\n",
    "rmse_pct_nn = rmse_nn / mean_charges * 100\n",
    "print(f\"MAE is about {mae_pct_nn:.1f}% of the average cost\")\n",
    "print(f\"RMSE is about {rmse_pct_nn:.1f}% of the average cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance for neural network\n",
    "r = permutation_importance(nn_model, x_test, y_test, n_repeats = 10, random_state = 42)\n",
    "\n",
    "sorted_idx = r.importances_mean.argsort()\n",
    "plt.barh(np.array(x_names)[sorted_idx], r.importances_mean[sorted_idx], color = palette)\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.title(\"Feature Importance (Neural Network)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding residual plots\n",
    "y_pred_nn = np.ravel(y_pred_nn)\n",
    "y_true_nn = np.ravel(y_true_nn)\n",
    "residuals = y_true_nn - y_pred_nn\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_pred_nn, y=residuals)\n",
    "plt.axhline(0, color='pink', linestyle='--') \n",
    "plt.xlabel('Predicted Charges')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot of Neural Network Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce150c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding actual vs predicted plot\n",
    "plt.scatter(y_true_nn, y_pred_nn, alpha = 0.4)\n",
    "plt.plot([y_true_nn.min(), y_true_nn.max()], [y_true_nn.min(), y_true_nn.max()], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f95393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding kfold cross val\n",
    "kfold = KFold(n_splits = 5, shuffle=True, random_state = 42)\n",
    "cv_scores = cross_val_score(nn_model, x_train, y_train, cv = kfold, scoring = 'r2')\n",
    "\n",
    "print(f\"Cross-Validation R² Scores: {cv_scores}\")\n",
    "print(f\"Mean R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning of error analysis\n",
    "errors = y_true_nn.flatten() - y_pred_nn.flatten()\n",
    "abs_errors = np.abs(errors)\n",
    "pct_errors = (abs_errors / y_true_nn.flatten()) * 100\n",
    "\n",
    "# create dataframe\n",
    "error_data_nn = pd.DataFrame({\n",
    "    'true_value': y_true_nn.flatten(),\n",
    "    'predicted_value': y_pred_nn.flatten(),\n",
    "    'error': errors,\n",
    "    'abs_error': abs_errors,\n",
    "    'pct_error': pct_errors\n",
    "})\n",
    "\n",
    "x_test_original = pd.DataFrame(\n",
    "    scaler.inverse_transform(x_test),\n",
    "    columns = x_names\n",
    ")\n",
    "\n",
    "error_data_nn = pd.concat([error_data_nn, x_test_original.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data_nn.to_csv('errors_nn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
